{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chest X-Ray Conditional GAN"
      ],
      "metadata": {
        "id": "GuQeQHHt71JP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Defined Parameters\n",
        "Set the following parameters before running"
      ],
      "metadata": {
        "id": "3Evi8nto_e6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to True if you want to load data and save models to Google Drive\n",
        "# Set to False if you want to load and save locally.\n",
        "load_data_from_Google_Drive = False\n",
        "\n",
        "workspace = '/content/mnt/Shareddrives/xray_cgan_260c/'\n",
        "model_name = 'conditional_final'\n",
        "data_location = workspace + \"data/cond_data.npy\"\n",
        "labels_location = workspace + \"data/cond_labels.npy\"\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "LATENT_DIM = 100\n",
        "INPUT_SHAPE = (128, 128, 1)\n",
        "LEARNING_RATE = 0.0002\n",
        "MOMENTUM = 0.5\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "classes_list = [\"No Finding\", \"Atelectasis\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumothorax\"]\n",
        "classes = {\"No Finding\": 0, \"Atelectasis\": 1, \"Effusion\": 2, \"Infiltration\": 3, \"Mass\": 4, \"Nodule\": 5, \"Pneumothorax\": 6}\n",
        "num_to_class = {v: k for k, v in classes.items()}"
      ],
      "metadata": {
        "id": "HuGto-cz3u1H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive if applicable"
      ],
      "metadata": {
        "id": "tRX92wmR_mDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMI8KWnuaWpt"
      },
      "outputs": [],
      "source": [
        "if load_data_from_Google_Drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/mnt', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "RzVdRzB9_rIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ukyBpRVAa7-o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from IPython import display\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator Definition"
      ],
      "metadata": {
        "id": "R6T1FF-y_ttX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LZeXtmVEbXue"
      },
      "outputs": [],
      "source": [
        "def define_discriminator(in_shape=(128, 128, 1), n_classes=7, lr=0.0002, momentum=0.5):\n",
        "  # label input\n",
        "  in_label = layers.Input(shape=(1,))\n",
        "  # embedding for categorical input\n",
        "  li = layers.Embedding(n_classes, 50)(in_label)\n",
        "\n",
        "  # Scale up\n",
        "  n_nodes = in_shape[0] * in_shape[1]\n",
        "  li = layers.Dense(n_nodes)(li)\n",
        "  li = layers.Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\n",
        "  # Input Image\n",
        "  in_image = layers.Input(shape=in_shape)\n",
        "\n",
        "  #Add in label\n",
        "  x = layers.Concatenate()([in_image, li])\n",
        "\n",
        "  # 128, 128, 2\n",
        "  x = layers.Conv2D(128, (5,5), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 64, 64, 128\n",
        "  \n",
        "  x = layers.Conv2D(128, (5,5), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 32, 32, 128\n",
        "  \n",
        "  x = layers.Conv2D(128, (5,5), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 16, 16, 128\n",
        "\n",
        "  x = layers.Conv2D(128, (5,5), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 8, 8, 128\n",
        "  \n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dropout(0.4)(x)\n",
        "  out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  # Define model\n",
        "  model = Model([in_image, in_label], out)\n",
        "\n",
        "  # Compile model\n",
        "  opt = Adam(learning_rate=lr, beta_1=momentum)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator Definition"
      ],
      "metadata": {
        "id": "Jr_-PbVP_wnT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b4JJm_yTeKAw"
      },
      "outputs": [],
      "source": [
        "def define_generator(latent_dim, n_classes=7):\n",
        "  # label input\n",
        "  in_label = layers.Input(shape=(1,))\n",
        "  # embedding for categorical input\n",
        "  li = layers.Embedding(n_classes, 50)(in_label)\n",
        "\n",
        "  # Scale\n",
        "  n_nodes = 8*8*64\n",
        "  li = layers.Dense(n_nodes)(li)\n",
        "  li = layers.Reshape((8,8,64))(li)\n",
        "\n",
        "  # Latent Space Input\n",
        "  in_lat = layers.Input(shape=(latent_dim, ))\n",
        "  n_nodes = 64*8*8\n",
        "  x = layers.Dense(n_nodes)(in_lat)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Reshape((8, 8, 64))(x)\n",
        "\n",
        "  # Merge image and label\n",
        "  x = layers.Concatenate()([x, li])\n",
        "  # 8, 8, 128\n",
        "  \n",
        "  x = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 16, 16, 128\n",
        "\n",
        "  x = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 32, 32, 128\n",
        "\n",
        "  x = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 64, 64, 128\n",
        "\n",
        "  x = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # 128, 128, 128\n",
        "\n",
        "  out = layers.Conv2D(1, (16,16), activation='tanh', padding='same')(x)\n",
        "\n",
        "  # define model\n",
        "  model = Model([in_lat, in_label], out)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Definition\n",
        "Both models sequentially for generator training"
      ],
      "metadata": {
        "id": "udX97MLF_zbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wrBo2bNBha1f"
      },
      "outputs": [],
      "source": [
        "def define_gan(generator, discriminator, lr=0.0002, momentum=0.5):\n",
        "  discriminator.trainable = False\n",
        "\n",
        "  generator_lat, generator_label = generator.input\n",
        "  generator_output = generator.output\n",
        "\n",
        "  gan_output = discriminator([generator_output, generator_label])\n",
        "\n",
        "  model = Model([generator_lat, generator_label], gan_output)\n",
        "\n",
        "  opt = Adam(learning_rate=lr, beta_1=momentum)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "tkbHXVyN_8EG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KGvzg9Sgh4n3"
      },
      "outputs": [],
      "source": [
        "def load_real_samples(X, labels):\n",
        "  X = np.expand_dims(X, axis=-1)\n",
        "  X = X.astype('float32')\n",
        "  X = (X - 127.5) / 127.5\n",
        "  return [X, labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gUt2mrNsm8eF"
      },
      "outputs": [],
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "  i = np.random.randint(0, dataset[0].shape[0], n_samples)\n",
        "  X = dataset[0][i]\n",
        "  labels = dataset[1][i]\n",
        "  y = np.ones((n_samples, 1))\n",
        "  \n",
        "  return [X, labels], y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XS9KBJHWnRTu"
      },
      "outputs": [],
      "source": [
        "def generate_latent_points(latent_dim, n_samples, n_classes=7):\n",
        "  x_input = np.random.randn(latent_dim*n_samples)\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  labels = np.random.randint(0, n_classes, n_samples)\n",
        "  return [x_input, labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r3BL_q-ookP4"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples, n_classes=7):\n",
        "  x_input, labels_input = generate_latent_points(latent_dim, n_samples, n_classes)\n",
        "  X = generator.predict([x_input, labels_input])\n",
        "  y = np.zeros((n_samples, 1))\n",
        "  \n",
        "  return [X, labels_input], y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OSfTvwYIpJyH"
      },
      "outputs": [],
      "source": [
        "def train(gan, discriminator, generator, dataset, latent_dim, n_classes, n_epochs=100, batch_size=128, starting_epoch=0):\n",
        "  batch_per_epoch = dataset[0].shape[0] // batch_size\n",
        "  half_batch = batch_size // 2\n",
        "\n",
        "  for epoch in range(starting_epoch, n_epochs+starting_epoch):\n",
        "    start = time.time()\n",
        "    for batch in range(batch_per_epoch):\n",
        "      batch_start = time.time()\n",
        "      # Real\n",
        "      [X, labels], y = generate_real_samples(dataset, half_batch)\n",
        "      d_loss1, _ = discriminator.train_on_batch([X,  labels], y)\n",
        "\n",
        "      # Fake\n",
        "      [X, labels], y = generate_fake_samples(generator, latent_dim, half_batch, n_classes)\n",
        "      d_loss2, _ = discriminator.train_on_batch([X, labels], y)\n",
        "\n",
        "      # GAN\n",
        "      [X, labels] = generate_latent_points(latent_dim, batch_size, n_classes)\n",
        "      y = np.ones((batch_size, 1))\n",
        "      g_loss = gan.train_on_batch([X, labels], y)\n",
        "      \n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (epoch+1, batch+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\n",
        "      print('Time for batch {}/{} of epoch {} is {} sec'.format(batch+1, batch_per_epoch, epoch + 1, time.time()-batch_start))\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    ### PERFORMANCE CHECK\n",
        "    X, y = generate_real_samples(dataset, 100) # real\n",
        "    _, acc_real = discriminator.evaluate(X, y, verbose=0)\n",
        "    X, y = generate_fake_samples(generator, latent_dim, 100, n_classes) # fake\n",
        "    _, acc_fake = discriminator.evaluate(X, y, verbose=0)\n",
        "    print(\"Disc Accuracy Real: %.5f, fake: %.5f\" % (acc_real, acc_fake))\n",
        "    print('Epoch %d: d1=%.3f, d2=%.3f g=%.3f' % (epoch+1, d_loss1, d_loss2, g_loss))\n",
        "\n",
        "    genloss.append(g_loss)\n",
        "    discloss.append((d_loss1+d_loss2)/2)\n",
        "    accReal.append(acc_real)\n",
        "    accFake.append(acc_fake)\n",
        "\n",
        "    generate_and_save_images(generator, discriminator,epoch + 1, np.random.randn(1, latent_dim), genloss, discloss, accReal, accFake, n_classes)\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "  \n",
        "  generator.save(workspace+\"/models/\"+model_name+\"/generator.h5\")\n",
        "  gan.save(workspace+\"/models/\"+model_name+\"/gan.h5\")\n",
        "  discriminator.save(workspace+\"/\"+model_name+\"/discriminator.h5\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/genloss.csv\", genloss, delimiter=\",\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/discloss.csv\", discloss, delimiter=\",\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/accreal.csv\", accReal, delimiter=\",\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/accfake.csv\", accFake, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cTJ47kHkDRFC"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, discriminator, epoch, test_input, gen_loss, disc_loss, acc_real, acc_fake, n_classes):\n",
        "  if epoch % 10 == 0:\n",
        "    model.save(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_generator.h5\")\n",
        "    discriminator.save(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_discriminator.h5\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_genloss.csv\", gen_loss, delimiter=\",\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_discloss.csv\", disc_loss, delimiter=\",\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_accreal.csv\", acc_real, delimiter=\",\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_accfake.csv\", acc_fake, delimiter=\",\")\n",
        "\n",
        "  fig, axs = plt.subplots(1, n_classes)\n",
        "  for label in range(n_classes):\n",
        "    predictions = model.predict([test_input, np.array([label])])\n",
        "\n",
        "    axs[label].imshow(predictions[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "    axs[label].axis('off')\n",
        "    axs[label].set_title(num_to_class[label], fontsize=10)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train The Model"
      ],
      "metadata": {
        "id": "cMg8qZ1W__lL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "b5p_aE6HAB51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "--zt8Fv8CIHq"
      },
      "outputs": [],
      "source": [
        "dataset = load_real_samples(np.load(data_location), np.load(labels_location))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "RzHH_7jOAELL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y3k6y3cWsygE"
      },
      "outputs": [],
      "source": [
        "discriminator = define_discriminator(in_shape=INPUT_SHAPE,n_classes=NUM_CLASSES, lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "generator = define_generator(LATENT_DIM, n_classes=NUM_CLASSES)\n",
        "gan = define_gan(generator=generator, discriminator=discriminator, lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "\n",
        "genloss = []\n",
        "discloss = []\n",
        "accReal = []\n",
        "accFake = []\n",
        "\n",
        "\n",
        "train(gan=gan, discriminator=discriminator, generator=generator, dataset=dataset, latent_dim=LATENT_DIM, n_classes=NUM_CLASSES, batch_size=BATCH_SIZE, n_epochs=NUM_EPOCHS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Results and Training Metrics"
      ],
      "metadata": {
        "id": "sNoM3If2AHh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(discloss)), discloss, label='disc loss')\n",
        "plt.plot(range(len(genloss)), genloss, label='gen loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(-1,5)\n",
        "plt.ylabel('Loss value') \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dUC-YLV8C3D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(accReal)), accReal, label='accuracy Real')\n",
        "plt.plot(range(len(accFake)), accFake, label='accuracy Fake')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arSZzeSUC5RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53svCjJltXsw"
      },
      "outputs": [],
      "source": [
        "def show_plot(label, n_samples, latent_dim=100, latent_space=np.random.randn(1,100)):\n",
        "  predictions = generator.predict([latent_space, np.array([label])])\n",
        "  plt.imshow(predictions[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(num_to_class[label])\n",
        "  plt.show()\n",
        "\n",
        "latent = np.random.randn(1,100)\n",
        "for i in range(NUM_CLASSES):\n",
        "  show_plot(i, 1, 100, latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxCWmVhHAeRD"
      },
      "outputs": [],
      "source": [
        "show_plot(generator.predict(generate_latent_points(100, 1)), 1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "conditionalGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
