{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chest X-Ray GAN (Non-conditional)"
      ],
      "metadata": {
        "id": "UVawtC3tAfiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Defined Parameters\n",
        "Set the following parameters before running"
      ],
      "metadata": {
        "id": "3Evi8nto_e6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to True if you want to load data and save models to Google Drive\n",
        "# Set to False if you want to load and save locally.\n",
        "load_data_from_Google_Drive = False\n",
        "\n",
        "workspace = '/content/mnt/Shareddrives/xray_cgan_260c/'\n",
        "model_name = 'regular_final'\n",
        "data_location = workspace + \"data/cond_data.npy\"\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "LATENT_DIM = 100\n",
        "INPUT_SHAPE = (128, 128, 1)\n",
        "LEARNING_RATE = 0.0002\n",
        "MOMENTUM = 0.5\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "RCcGNfc4AqC5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive if applicable"
      ],
      "metadata": {
        "id": "tRX92wmR_mDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMI8KWnuaWpt"
      },
      "outputs": [],
      "source": [
        "if load_data_from_Google_Drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/mnt', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "3Y5bdUcbA5Zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ukyBpRVAa7-o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from IPython import display\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator Definition"
      ],
      "metadata": {
        "id": "QwYXatQ8A8fn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LZeXtmVEbXue"
      },
      "outputs": [],
      "source": [
        "def define_discriminator(in_shape=(128, 128, 1), lr=0.0002, momentum=0.5):\n",
        "  model = Sequential()\n",
        "\n",
        "  # 128, 128, 1\n",
        "  model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same', input_shape=in_shape))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 64, 64, 128\n",
        "  \n",
        "  model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 32, 32, 128\n",
        "  \n",
        "  model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 16, 16, 128\n",
        "\n",
        "  model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 8, 8, 64\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  # Compile model\n",
        "  opt = Adam(learning_rate=lr, beta_1=momentum)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator Definition"
      ],
      "metadata": {
        "id": "v3RCkBGUA-aN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b4JJm_yTeKAw"
      },
      "outputs": [],
      "source": [
        "def define_generator(latent_dim):\n",
        "  model = Sequential()\n",
        "\n",
        "  n_nodes = 64*8*8\n",
        "  model.add(layers.Dense(n_nodes, input_dim=latent_dim))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Reshape((8, 8, 64)))\n",
        "  # 8, 8, 64\n",
        "  \n",
        "  model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 16, 16, 128\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 32, 32, 128\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 64, 64, 128\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # 128, 128, 128\n",
        "\n",
        "  model.add(layers.Conv2D(1, (16,16), activation='tanh', padding='same'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Definition\n",
        "Both models sequentially for generator training"
      ],
      "metadata": {
        "id": "udX97MLF_zbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wrBo2bNBha1f"
      },
      "outputs": [],
      "source": [
        "def define_gan(generator, discriminator, lr=0.0002, momentum=0.5):\n",
        "  discriminator.trainable = False\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "\n",
        "  opt = Adam(learning_rate=lr, beta_1=momentum)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "0PSulXeaBFF9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KGvzg9Sgh4n3"
      },
      "outputs": [],
      "source": [
        "def load_real_samples(X):\n",
        "  X = np.expand_dims(X, axis=-1)\n",
        "  X = X.astype('float32')\n",
        "  X = (X - 127.5) / 127.5\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gUt2mrNsm8eF"
      },
      "outputs": [],
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "  i = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  X = dataset[i]\n",
        "  y = np.ones((n_samples, 1))\n",
        "  \n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XS9KBJHWnRTu"
      },
      "outputs": [],
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  x_input = np.random.randn(latent_dim*n_samples)\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r3BL_q-ookP4"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\n",
        "  X = generator.predict(x_input)\n",
        "  y = np.zeros((n_samples, 1))\n",
        "  \n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OSfTvwYIpJyH"
      },
      "outputs": [],
      "source": [
        "def train(gan, discriminator, generator, dataset, latent_dim, n_epochs=100, batch_size=128, starting_epoch=0):\n",
        "  batch_per_epoch = dataset.shape[0] // batch_size\n",
        "  half_batch = batch_size // 2\n",
        "\n",
        "  for epoch in range(starting_epoch, n_epochs+starting_epoch):\n",
        "    start = time.time()\n",
        "    for batch in range(batch_per_epoch):\n",
        "      batch_start = time.time()\n",
        "      # Real\n",
        "      X, y = generate_real_samples(dataset, half_batch)\n",
        "      d_loss1, _ = discriminator.train_on_batch(X, y)\n",
        "\n",
        "      # Fake\n",
        "      X, y = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "      d_loss2, _ = discriminator.train_on_batch(X, y)\n",
        "\n",
        "      # GAN\n",
        "      X = generate_latent_points(latent_dim, batch_size)\n",
        "      y = np.ones((batch_size, 1))\n",
        "      g_loss = gan.train_on_batch(X, y)\n",
        "      \n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (epoch+1, batch+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\n",
        "      print('Time for batch {}/{} of epoch {} is {} sec'.format(batch+1, batch_per_epoch, epoch + 1, time.time()-batch_start))\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    ### PERFORMANCE CHECK\n",
        "    X, y = generate_real_samples(dataset, 100) # real\n",
        "    _, acc_real = discriminator.evaluate(X, y, verbose=0)\n",
        "    X, y = generate_fake_samples(generator, latent_dim, 100) # fake\n",
        "    _, acc_fake = discriminator.evaluate(X, y, verbose=0)\n",
        "    print(\"Disc Accuracy Real: %.5f, fake: %.5f\" % (acc_real, acc_fake))\n",
        "    print('Epoch %d: d1=%.3f, d2=%.3f g=%.3f' % (epoch+1, d_loss1, d_loss2, g_loss))\n",
        "\n",
        "    genloss.append(g_loss)\n",
        "    discloss.append((d_loss1+d_loss2)/2)\n",
        "    accReal.append(acc_real)\n",
        "    accFake.append(acc_fake)\n",
        "\n",
        "    generate_and_save_images(generator, discriminator,epoch + 1, np.random.randn(1, latent_dim), genloss, discloss, accReal, accFake)\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))  \n",
        "  \n",
        "  generator.save(workspace+\"/models/\"+model_name+\"/generator.h5\")\n",
        "  gan.save(workspace+\"/models/\"+model_name+\"/gan.h5\")\n",
        "  discriminator.save(workspace+\"/\"+model_name+\"/discriminator.h5\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/genloss.csv\", genloss, delimiter=\",\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/discloss.csv\", discloss, delimiter=\",\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/accreal.csv\", accReal, delimiter=\",\")\n",
        "  np.savetxt(workspace+\"/models/\"+model_name+\"/accfake.csv\", accFake, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cTJ47kHkDRFC"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, discriminator, epoch, test_input, gen_loss, disc_loss, acc_real, acc_fake):\n",
        "  if epoch % 10 == 0:\n",
        "    model.save(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_generator.h5\")\n",
        "    discriminator.save(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_discriminator.h5\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_genloss.csv\", gen_loss, delimiter=\",\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_discloss.csv\", disc_loss, delimiter=\",\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_accreal.csv\", acc_real, delimiter=\",\")\n",
        "    np.savetxt(workspace+\"/models/\"+model_name+\"/intermediate\"+str(epoch)+\"_accfake.csv\", acc_fake, delimiter=\",\")\n",
        "\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "8ToWF8iaB6Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "Kahm6pKsB8TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "--zt8Fv8CIHq"
      },
      "outputs": [],
      "source": [
        "dataset = load_real_samples(np.load(data_location))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "7qJ8uOKnB-O6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3k6y3cWsygE"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100\n",
        "discriminator = define_discriminator()\n",
        "generator = define_generator(latent_dim)\n",
        "gan = define_gan(generator=generator, discriminator=discriminator)\n",
        "\n",
        "genloss = []\n",
        "discloss = []\n",
        "accReal = []\n",
        "accFake = []\n",
        "\n",
        "#dataset = load_real_samples(data)\n",
        "\n",
        "train(gan=gan, discriminator=discriminator, generator=generator, dataset=dataset, latent_dim=latent_dim, n_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Results and Training Metrics"
      ],
      "metadata": {
        "id": "sNoM3If2AHh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(discloss)), discloss, label='disc loss')\n",
        "plt.plot(range(len(genloss)), genloss, label='gen loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(-1,5)\n",
        "plt.ylabel('Loss value') \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dUC-YLV8C3D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(accReal)), accReal, label='accuracy Real')\n",
        "plt.plot(range(len(accFake)), accFake, label='accuracy Fake')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arSZzeSUC5RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53svCjJltXsw"
      },
      "outputs": [],
      "source": [
        "# example of loading the generator model and generating images\n",
        "from keras.models import load_model\n",
        "from numpy.random import randn\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def show_plot(examples, n):\n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "    # define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    # turn off axis\n",
        "    pyplot.axis('off')\n",
        "    # plot raw pixel data\n",
        "    pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "  pyplot.show()\n",
        " \n",
        "model = generator\n",
        "latent_points = generate_latent_points(100, 100)\n",
        "X = model.predict(latent_points)\n",
        "show_plot(X, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxCWmVhHAeRD"
      },
      "outputs": [],
      "source": [
        "show_plot(model.predict(generate_latent_points(100, 1)), 1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "regularGAN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
